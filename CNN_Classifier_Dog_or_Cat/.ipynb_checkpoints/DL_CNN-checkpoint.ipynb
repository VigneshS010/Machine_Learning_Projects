{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1Rc9Ncf62dc"
   },
   "source": [
    "Predicting the new image given are Dog or Cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XvbwWUJ7bzE"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vztq9kmI7bX3"
   },
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1723354818560,
     "user": {
      "displayName": "Vignesh S",
      "userId": "03378886263089425882"
     },
     "user_tz": -330
    },
    "id": "D3cAVC-qoJdu"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# keras is moved into tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1723354804609,
     "user": {
      "displayName": "Vignesh S",
      "userId": "03378886263089425882"
     },
     "user_tz": -330
    },
    "id": "Mq-7MSho70aw",
    "outputId": "61c26069-4b55-48c7-8bc7-d5b3d887759f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.17.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVaz706Q9Rco"
   },
   "source": [
    "## Part 1 Data PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxXD-Jld9cFC"
   },
   "source": [
    "Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0cJg7VV9kQ4"
   },
   "source": [
    "The Transformation is applied to the dataset, because if we doest use it , it becomes overfitting\n",
    "To avoid that kind of issues transformation are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1723356224353,
     "user": {
      "displayName": "Vignesh S",
      "userId": "03378886263089425882"
     },
     "user_tz": -330
    },
    "id": "ETEMQl589V8K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Transformations include flip, rotate, move etc..\n",
    "\n",
    "# rescala is Feature Scaling\n",
    "# It feature scale every single pixels on the picture\n",
    "# Fearture scaling is compulsory in the Deep Learning\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True\n",
    ")\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    'dataset/training_set',\n",
    "# (150,150) it takes more time so we use 64\n",
    "    target_size = (64,64),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwFnbXUUCUn4"
   },
   "source": [
    "PreProcessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_V6v1HFHAU3u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'dataset/test_set',\n",
    "    target_size = (64,64),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I75S7aIrC2om"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYjvAMZWDRDt"
   },
   "source": [
    "Initializing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1723356516276,
     "user": {
      "displayName": "Vignesh S",
      "userId": "03378886263089425882"
     },
     "user_tz": -330
    },
    "id": "9NZfqJkOC527"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGS7VjYIDZe-"
   },
   "source": [
    "Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1723359702467,
     "user": {
      "displayName": "Vignesh S",
      "userId": "03378886263089425882"
     },
     "user_tz": -330
    },
    "id": "QwctomvHDYub",
    "outputId": "f872665e-0f4c-4cf1-b7fc-73991abc17d9"
   },
   "outputs": [],
   "source": [
    "# we choosed 64 target size\n",
    "# so here 32 and second conv layer have 32\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jOnpKwkDdyC"
   },
   "source": [
    "Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 656,
     "status": "ok",
     "timestamp": 1723360045858,
     "user": {
      "displayName": "Vignesh S",
      "userId": "03378886263089425882"
     },
     "user_tz": -330
    },
    "id": "cbdX7mysDg-O"
   },
   "outputs": [],
   "source": [
    "# Max pooling we using here\n",
    "# pool size = 2 X 2 , so we choosed 2\n",
    "# stride refers the shift of next matrix 0,1 to 2,3 for next val, so no repeat occurs\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peGr8T4iDhgC"
   },
   "source": [
    "Adding a second convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1723360118239,
     "user": {
      "displayName": "Vignesh S",
      "userId": "03378886263089425882"
     },
     "user_tz": -330
    },
    "id": "Dsl7O7_QDm_G"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters= 32, kernel_size = 3, activation = 'relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz6VfXl5DrVq"
   },
   "source": [
    "Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1723360199526,
     "user": {
      "displayName": "Vignesh S",
      "userId": "03378886263089425882"
     },
     "user_tz": -330
    },
    "id": "n8NAMsrDDulT"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhVpWTR0DvFl"
   },
   "source": [
    "Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1723360297208,
     "user": {
      "displayName": "Vignesh S",
      "userId": "03378886263089425882"
     },
     "user_tz": -330
    },
    "id": "vIGMLX0FDxvf"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units= 128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRmitEwpDyOO"
   },
   "source": [
    "Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 422,
     "status": "ok",
     "timestamp": 1723360339356,
     "user": {
      "displayName": "Vignesh S",
      "userId": "03378886263089425882"
     },
     "user_tz": -330
    },
    "id": "1A7S2_XCR7Ti"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units= 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIrhcZQeR_IA"
   },
   "source": [
    "## Step 3 - Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSZ_0knXSeSl"
   },
   "source": [
    "Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5Ywr9KC3R-GT"
   },
   "outputs": [],
   "source": [
    "# compile have optimizer, loss and metrics\n",
    "# loss function is also called cost function\n",
    "\n",
    "# adam - recommended also here\n",
    "# binary_crossentropy - binary output so , if categorical , categorical_crossentropy is used\n",
    "\n",
    "cnn.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8caNnXrTAdZ"
   },
   "source": [
    "Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in c:\\users\\vignesh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.14.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in c:\\users\\vignesh\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scipy) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "HHD5QYTQTIqA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 309ms/step - accuracy: 0.5442 - loss: 0.6899 - val_accuracy: 0.5285 - val_loss: 0.6862\n",
      "Epoch 2/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 151ms/step - accuracy: 0.5974 - loss: 0.6615 - val_accuracy: 0.6665 - val_loss: 0.6262\n",
      "Epoch 3/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 142ms/step - accuracy: 0.6796 - loss: 0.6057 - val_accuracy: 0.6975 - val_loss: 0.5860\n",
      "Epoch 4/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 139ms/step - accuracy: 0.7072 - loss: 0.5783 - val_accuracy: 0.7345 - val_loss: 0.5519\n",
      "Epoch 5/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 142ms/step - accuracy: 0.7276 - loss: 0.5338 - val_accuracy: 0.7440 - val_loss: 0.5251\n",
      "Epoch 6/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 135ms/step - accuracy: 0.7493 - loss: 0.5087 - val_accuracy: 0.7530 - val_loss: 0.5131\n",
      "Epoch 7/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 126ms/step - accuracy: 0.7713 - loss: 0.4750 - val_accuracy: 0.7820 - val_loss: 0.4779\n",
      "Epoch 8/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 123ms/step - accuracy: 0.7819 - loss: 0.4524 - val_accuracy: 0.7655 - val_loss: 0.4855\n",
      "Epoch 9/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 116ms/step - accuracy: 0.7926 - loss: 0.4384 - val_accuracy: 0.7860 - val_loss: 0.4599\n",
      "Epoch 10/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 122ms/step - accuracy: 0.8042 - loss: 0.4194 - val_accuracy: 0.8040 - val_loss: 0.4448\n",
      "Epoch 11/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 122ms/step - accuracy: 0.8123 - loss: 0.4075 - val_accuracy: 0.8040 - val_loss: 0.4320\n",
      "Epoch 12/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 123ms/step - accuracy: 0.8204 - loss: 0.3845 - val_accuracy: 0.7900 - val_loss: 0.4460\n",
      "Epoch 13/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 116ms/step - accuracy: 0.8351 - loss: 0.3692 - val_accuracy: 0.8095 - val_loss: 0.4191\n",
      "Epoch 14/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 153ms/step - accuracy: 0.8489 - loss: 0.3500 - val_accuracy: 0.8005 - val_loss: 0.4389\n",
      "Epoch 15/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 150ms/step - accuracy: 0.8453 - loss: 0.3466 - val_accuracy: 0.8105 - val_loss: 0.4296\n",
      "Epoch 16/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 133ms/step - accuracy: 0.8533 - loss: 0.3366 - val_accuracy: 0.8215 - val_loss: 0.4359\n",
      "Epoch 17/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 138ms/step - accuracy: 0.8690 - loss: 0.3113 - val_accuracy: 0.8085 - val_loss: 0.4426\n",
      "Epoch 18/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 124ms/step - accuracy: 0.8594 - loss: 0.3151 - val_accuracy: 0.8065 - val_loss: 0.4528\n",
      "Epoch 19/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 144ms/step - accuracy: 0.8604 - loss: 0.3084 - val_accuracy: 0.8070 - val_loss: 0.4698\n",
      "Epoch 20/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 141ms/step - accuracy: 0.8774 - loss: 0.2812 - val_accuracy: 0.8100 - val_loss: 0.4548\n",
      "Epoch 21/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 133ms/step - accuracy: 0.8871 - loss: 0.2746 - val_accuracy: 0.8305 - val_loss: 0.4902\n",
      "Epoch 22/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 133ms/step - accuracy: 0.8805 - loss: 0.2792 - val_accuracy: 0.8295 - val_loss: 0.4650\n",
      "Epoch 23/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 132ms/step - accuracy: 0.8953 - loss: 0.2504 - val_accuracy: 0.8175 - val_loss: 0.4863\n",
      "Epoch 24/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 123ms/step - accuracy: 0.8983 - loss: 0.2372 - val_accuracy: 0.8035 - val_loss: 0.5080\n",
      "Epoch 25/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 127ms/step - accuracy: 0.8974 - loss: 0.2440 - val_accuracy: 0.8220 - val_loss: 0.5061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x25800eb9850>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recommended batch size 32\n",
    "# epochs  need to be some large number\n",
    "# Here we train and evaluate in same set, so we use validation data\n",
    "\n",
    "cnn.fit(x= training_set, validation_data=test_set, epochs= 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjFiO2W3T3ZS"
   },
   "source": [
    "## Part-4 Making a single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "LjzsEjLkT_ho"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "test_image = image.load_img('single_prediction/cat2.jpg', target_size = (64,64))\n",
    "\n",
    "# predict expect 2D array , so covert PIL(image) format to array\n",
    "test_image = image.img_to_array(test_image)\n",
    "\n",
    "# above all in 32 per batches , so here we only one image for test\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "\n",
    "training_set.class_indices\n",
    "\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'\n",
    "\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNdLYUYD32sOfGNAdKVT3GR",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
